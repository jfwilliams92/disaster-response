{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Disaster Reponse Pipeline Creation \n",
    "\n",
    "\n",
    "### 1. Import libraries and load data from database.\n",
    "- Import Python libraries\n",
    "- Load dataset from database with [`read_sql_table`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_sql_table.html)\n",
    "- Define feature and target variables X and Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/james/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/james/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/james/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to /home/james/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# import libraries\n",
    "import pandas as pd\n",
    "import sqlite3\n",
    "import re\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from sqlalchemy import create_engine\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "from skmultilearn.model_selection import iterative_train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data from database\n",
    "engine = create_engine('sqlite:///DisasterMessages.db')\n",
    "df = pd.read_sql('SELECT * FROM CleanMessages', engine)\n",
    "\n",
    "# define variables. X is input, Y is target\n",
    "X = df['message']\n",
    "Y = df.drop(['id', 'message', 'original', 'genre'], axis=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the 'child alone' tag because there are no messages with this tag.\n",
    "Y = Y.drop('child_alone', axis=1)\n",
    "\n",
    "# replace 2's with 1's in the related field\n",
    "Y.loc[Y.related == 2, 'related'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>message</th>\n",
       "      <th>original</th>\n",
       "      <th>genre</th>\n",
       "      <th>related</th>\n",
       "      <th>request</th>\n",
       "      <th>offer</th>\n",
       "      <th>aid_related</th>\n",
       "      <th>medical_help</th>\n",
       "      <th>medical_products</th>\n",
       "      <th>...</th>\n",
       "      <th>aid_centers</th>\n",
       "      <th>other_infrastructure</th>\n",
       "      <th>weather_related</th>\n",
       "      <th>floods</th>\n",
       "      <th>storm</th>\n",
       "      <th>fire</th>\n",
       "      <th>earthquake</th>\n",
       "      <th>cold</th>\n",
       "      <th>other_weather</th>\n",
       "      <th>direct_report</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>Weather update - a cold front from Cuba that c...</td>\n",
       "      <td>Un front froid se retrouve sur Cuba ce matin. ...</td>\n",
       "      <td>direct</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7</td>\n",
       "      <td>Is the Hurricane over or is it not over</td>\n",
       "      <td>Cyclone nan fini osinon li pa fini</td>\n",
       "      <td>direct</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>Looking for someone but no name</td>\n",
       "      <td>Patnm, di Maryani relem pou li banm nouvel li ...</td>\n",
       "      <td>direct</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>UN reports Leogane 80-90 destroyed. Only Hospi...</td>\n",
       "      <td>UN reports Leogane 80-90 destroyed. Only Hospi...</td>\n",
       "      <td>direct</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12</td>\n",
       "      <td>says: west side of Haiti, rest of the country ...</td>\n",
       "      <td>facade ouest d Haiti et le reste du pays aujou...</td>\n",
       "      <td>direct</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>14</td>\n",
       "      <td>Information about the National Palace-</td>\n",
       "      <td>Informtion au nivaux palais nationl</td>\n",
       "      <td>direct</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>15</td>\n",
       "      <td>Storm at sacred heart of jesus</td>\n",
       "      <td>Cyclone Coeur sacr de jesus</td>\n",
       "      <td>direct</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>16</td>\n",
       "      <td>Please, we need tents and water. We are in Sil...</td>\n",
       "      <td>Tanpri nou bezwen tant avek dlo nou zon silo m...</td>\n",
       "      <td>direct</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>17</td>\n",
       "      <td>I would like to receive the messages, thank you</td>\n",
       "      <td>Mwen ta renmen jouin messag yo. Merci</td>\n",
       "      <td>direct</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>18</td>\n",
       "      <td>I am in Croix-des-Bouquets. We have health iss...</td>\n",
       "      <td>Nou kwadebouke, nou gen pwoblem sant m yo nan ...</td>\n",
       "      <td>direct</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows Ã— 40 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                            message  \\\n",
       "0   2  Weather update - a cold front from Cuba that c...   \n",
       "1   7            Is the Hurricane over or is it not over   \n",
       "2   8                    Looking for someone but no name   \n",
       "3   9  UN reports Leogane 80-90 destroyed. Only Hospi...   \n",
       "4  12  says: west side of Haiti, rest of the country ...   \n",
       "5  14             Information about the National Palace-   \n",
       "6  15                     Storm at sacred heart of jesus   \n",
       "7  16  Please, we need tents and water. We are in Sil...   \n",
       "8  17    I would like to receive the messages, thank you   \n",
       "9  18  I am in Croix-des-Bouquets. We have health iss...   \n",
       "\n",
       "                                            original   genre  related  \\\n",
       "0  Un front froid se retrouve sur Cuba ce matin. ...  direct        1   \n",
       "1                 Cyclone nan fini osinon li pa fini  direct        1   \n",
       "2  Patnm, di Maryani relem pou li banm nouvel li ...  direct        1   \n",
       "3  UN reports Leogane 80-90 destroyed. Only Hospi...  direct        1   \n",
       "4  facade ouest d Haiti et le reste du pays aujou...  direct        1   \n",
       "5                Informtion au nivaux palais nationl  direct        0   \n",
       "6                        Cyclone Coeur sacr de jesus  direct        1   \n",
       "7  Tanpri nou bezwen tant avek dlo nou zon silo m...  direct        1   \n",
       "8              Mwen ta renmen jouin messag yo. Merci  direct        0   \n",
       "9  Nou kwadebouke, nou gen pwoblem sant m yo nan ...  direct        1   \n",
       "\n",
       "   request  offer  aid_related  medical_help  medical_products  ...  \\\n",
       "0        0      0            0             0                 0  ...   \n",
       "1        0      0            1             0                 0  ...   \n",
       "2        0      0            0             0                 0  ...   \n",
       "3        1      0            1             0                 1  ...   \n",
       "4        0      0            0             0                 0  ...   \n",
       "5        0      0            0             0                 0  ...   \n",
       "6        0      0            0             0                 0  ...   \n",
       "7        1      0            1             0                 0  ...   \n",
       "8        0      0            0             0                 0  ...   \n",
       "9        1      0            1             1                 1  ...   \n",
       "\n",
       "   aid_centers  other_infrastructure  weather_related  floods  storm  fire  \\\n",
       "0            0                     0                0       0      0     0   \n",
       "1            0                     0                1       0      1     0   \n",
       "2            0                     0                0       0      0     0   \n",
       "3            0                     0                0       0      0     0   \n",
       "4            0                     0                0       0      0     0   \n",
       "5            0                     0                0       0      0     0   \n",
       "6            0                     0                1       0      1     0   \n",
       "7            0                     0                0       0      0     0   \n",
       "8            0                     0                0       0      0     0   \n",
       "9            0                     0                0       0      0     0   \n",
       "\n",
       "   earthquake  cold  other_weather  direct_report  \n",
       "0           0     0              0              0  \n",
       "1           0     0              0              0  \n",
       "2           0     0              0              0  \n",
       "3           0     0              0              0  \n",
       "4           0     0              0              0  \n",
       "5           0     0              0              0  \n",
       "6           0     0              0              0  \n",
       "7           0     0              0              1  \n",
       "8           0     0              0              0  \n",
       "9           0     0              0              1  \n",
       "\n",
       "[10 rows x 40 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# original data\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "188"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# how many in original data are related=2\n",
    "(df.related == 2).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((26216,), (26216, 35))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check out the shape of the data\n",
    "X.shape, Y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We have 26k messages with 35 possible labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Write a tokenization class to process the text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function that will allow a treebank POS tag to be converted into a WordNet\n",
    "# POS Tag so the lemmatizer will understand it\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    # default to Noun \n",
    "    else:\n",
    "        return wordnet.NOUN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will implement a custom transformer with hyperparameters in order to determine if removing stop words and/or lemmatizing words will have a positive effect on classification efficacy.\n",
    "This custom transformer follows sklearn's rules for transformers so that it can be used in our processing pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implement a custom transformer to determine if removing stops and/or lemmatizing improves model performance\n",
    "\n",
    "class MessageTokenizer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, remove_stops=True, lemmatize=True):\n",
    "        self.remove_stops = remove_stops\n",
    "        self.lemmatize = lemmatize\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        X_transformed = []\n",
    "        \n",
    "        # iterate over supplied messages\n",
    "        for text in X: \n",
    "            # remove all non-alphanumeric characters\n",
    "            text = re.sub(r'[^a-zA-Z0-9]', ' ', text)\n",
    "              \n",
    "            # lower and strip whitespace\n",
    "            text = text.lower().strip()\n",
    "    \n",
    "            # tokenize words - nltk.tokenize.word_tokenize\n",
    "            words = word_tokenize(text)\n",
    "            \n",
    "            if self.lemmatize:\n",
    "                # tag words with Part of Speech - list of (word, POS) tuples \n",
    "                # nltk.pos_tag()\n",
    "                words_with_pos_tag = pos_tag(words)\n",
    "                \n",
    "                if self.remove_stops:\n",
    "                    # remove stop words\n",
    "                    # stop_words = nlt.corpus.stopwords of 'english' language\n",
    "                    words_with_pos_tag = [word for word in words_with_pos_tag if word[0] not in stop_words]\n",
    "                \n",
    "                # change pos tags to wordnet pos tags for lemmatizer\n",
    "                words_with_wordnet_tag = []\n",
    "    \n",
    "                for word_with_tag in words_with_pos_tag:\n",
    "                    word, tag = word_with_tag\n",
    "                    tag = get_wordnet_pos(tag)\n",
    "                    words_with_wordnet_tag.append((word, tag))\n",
    "\n",
    "                # lemmatize\n",
    "                lemm = WordNetLemmatizer()\n",
    "                # unpack the (word, pos) tuple into the Lemmatizer to give better lemmatization \n",
    "                # lemmatization is more effective when it knows the correct part of speech\n",
    "                words = [lemm.lemmatize(*w) for w in words_with_wordnet_tag]\n",
    "                \n",
    "            else:\n",
    "                if self.remove_stops:\n",
    "                    words = [word for word in words if word not in stop_words]\n",
    "\n",
    "            # join cleaned words back into single document\n",
    "            X_transformed.append(' '.join(words))\n",
    "        \n",
    "        return X_transformed    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show off the custom transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['We would not want these words taking up space in our database, or taking up valuable processing time. For this, we can remove them easily, by storing a list of words that you consider to be stop words',\n",
       " \"Here is another example of words. Isn't it great how words are?\"]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = [\"We would not want these words taking up space in our database, or taking up valuable processing time. For this, we can remove them easily, by storing a list of words that you consider to be stop words\"]\n",
    "text.append(\"Here is another example of words. Isn't it great how words are?\")\n",
    "\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['we would not want these word take up space in our database or take up valuable processing time for this we can remove them easily by store a list of word that you consider to be stop word',\n",
       " 'here be another example of word isn t it great how word be']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MessageTokenizer(remove_stops=False, lemmatize=True).transform(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Build a first machine learning pipeline\n",
    "A pipeline allows us to wrap all of our data transformation steps into a neat little package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('msg_tokenizer', MessageTokenizer()),\n",
    "    # Count Vectorizer with Tokenizer\n",
    "    ('count_vec', CountVectorizer()),\n",
    "    # TF-IDF Transformer\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    # classifier - one classifier per label\n",
    "    ('clf', RandomForestClassifier())\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Train pipeline\n",
    "- Split data into train and test sets\n",
    "- Train pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since this is a multilabel classification, I looked into the iterative train-test-split supplied by skmultilearn.\n",
    "The idea with the iterative train test split is that in theory it can provide better label representation for mutli label problems. Here I will compare whether this train test split results in appropriate label representation for the train set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the proportion of labels in the original data\n",
    "compare = pd.DataFrame(Y.mean(axis=0), columns=['dataset'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>related</th>\n",
       "      <td>0.766478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>request</th>\n",
       "      <td>0.170659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>offer</th>\n",
       "      <td>0.004501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aid_related</th>\n",
       "      <td>0.414251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>medical_help</th>\n",
       "      <td>0.079493</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               dataset\n",
       "related       0.766478\n",
       "request       0.170659\n",
       "offer         0.004501\n",
       "aid_related   0.414251\n",
       "medical_help  0.079493"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compare.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# employ skmultilearn's iterative train test split.\n",
    "# have to reshape the X values to be multidimensional since that's what this expects\n",
    "\n",
    "X_train, y_train, X_test, y_test = iterative_train_test_split(X.values.reshape(-1,1), Y.values, test_size = 0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we want to see how the iterative split did with label proportions\n",
    "compare['train_set'] = y_train.mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normal train test split - how does it compare\n",
    "X_train2, X_test2, y_train2, y_test2 = train_test_split(X, Y, test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare['normal_split'] = y_train2.values.mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th>train_set</th>\n",
       "      <th>normal_split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>related</th>\n",
       "      <td>0.766478</td>\n",
       "      <td>0.766453</td>\n",
       "      <td>0.767877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>request</th>\n",
       "      <td>0.170659</td>\n",
       "      <td>0.136100</td>\n",
       "      <td>0.171041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>offer</th>\n",
       "      <td>0.004501</td>\n",
       "      <td>0.004476</td>\n",
       "      <td>0.004832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aid_related</th>\n",
       "      <td>0.414251</td>\n",
       "      <td>0.414251</td>\n",
       "      <td>0.415675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>medical_help</th>\n",
       "      <td>0.079493</td>\n",
       "      <td>0.087682</td>\n",
       "      <td>0.080155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>medical_products</th>\n",
       "      <td>0.050084</td>\n",
       "      <td>0.053962</td>\n",
       "      <td>0.049537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>search_and_rescue</th>\n",
       "      <td>0.027617</td>\n",
       "      <td>0.028380</td>\n",
       "      <td>0.027820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>security</th>\n",
       "      <td>0.017966</td>\n",
       "      <td>0.019733</td>\n",
       "      <td>0.017953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>military</th>\n",
       "      <td>0.032804</td>\n",
       "      <td>0.041552</td>\n",
       "      <td>0.032347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>water</th>\n",
       "      <td>0.063778</td>\n",
       "      <td>0.062303</td>\n",
       "      <td>0.064490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>food</th>\n",
       "      <td>0.111497</td>\n",
       "      <td>0.098515</td>\n",
       "      <td>0.111179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>shelter</th>\n",
       "      <td>0.088267</td>\n",
       "      <td>0.084986</td>\n",
       "      <td>0.088394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>clothing</th>\n",
       "      <td>0.015449</td>\n",
       "      <td>0.013478</td>\n",
       "      <td>0.015817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>money</th>\n",
       "      <td>0.023039</td>\n",
       "      <td>0.023446</td>\n",
       "      <td>0.023090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>missing_people</th>\n",
       "      <td>0.011367</td>\n",
       "      <td>0.010884</td>\n",
       "      <td>0.010935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>refugees</th>\n",
       "      <td>0.033377</td>\n",
       "      <td>0.038246</td>\n",
       "      <td>0.031940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>death</th>\n",
       "      <td>0.045545</td>\n",
       "      <td>0.050554</td>\n",
       "      <td>0.046638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>other_aid</th>\n",
       "      <td>0.131446</td>\n",
       "      <td>0.123538</td>\n",
       "      <td>0.131472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>infrastructure_related</th>\n",
       "      <td>0.065037</td>\n",
       "      <td>0.068355</td>\n",
       "      <td>0.064490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>transport</th>\n",
       "      <td>0.045812</td>\n",
       "      <td>0.047961</td>\n",
       "      <td>0.046536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>buildings</th>\n",
       "      <td>0.050847</td>\n",
       "      <td>0.050860</td>\n",
       "      <td>0.051978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>electricity</th>\n",
       "      <td>0.020293</td>\n",
       "      <td>0.020293</td>\n",
       "      <td>0.019937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tools</th>\n",
       "      <td>0.006065</td>\n",
       "      <td>0.006764</td>\n",
       "      <td>0.006052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hospitals</th>\n",
       "      <td>0.010795</td>\n",
       "      <td>0.011647</td>\n",
       "      <td>0.010681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>shops</th>\n",
       "      <td>0.004577</td>\n",
       "      <td>0.004425</td>\n",
       "      <td>0.004933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aid_centers</th>\n",
       "      <td>0.011787</td>\n",
       "      <td>0.012359</td>\n",
       "      <td>0.011342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>other_infrastructure</th>\n",
       "      <td>0.043904</td>\n",
       "      <td>0.046740</td>\n",
       "      <td>0.043637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weather_related</th>\n",
       "      <td>0.278341</td>\n",
       "      <td>0.278354</td>\n",
       "      <td>0.279575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>floods</th>\n",
       "      <td>0.082202</td>\n",
       "      <td>0.093175</td>\n",
       "      <td>0.082901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>storm</th>\n",
       "      <td>0.093187</td>\n",
       "      <td>0.091649</td>\n",
       "      <td>0.093327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fire</th>\n",
       "      <td>0.010757</td>\n",
       "      <td>0.012206</td>\n",
       "      <td>0.010477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>earthquake</th>\n",
       "      <td>0.093645</td>\n",
       "      <td>0.081782</td>\n",
       "      <td>0.093582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cold</th>\n",
       "      <td>0.020217</td>\n",
       "      <td>0.022124</td>\n",
       "      <td>0.020496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>other_weather</th>\n",
       "      <td>0.052487</td>\n",
       "      <td>0.058285</td>\n",
       "      <td>0.052894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>direct_report</th>\n",
       "      <td>0.193584</td>\n",
       "      <td>0.157359</td>\n",
       "      <td>0.196216</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         dataset  train_set  normal_split\n",
       "related                 0.766478   0.766453      0.767877\n",
       "request                 0.170659   0.136100      0.171041\n",
       "offer                   0.004501   0.004476      0.004832\n",
       "aid_related             0.414251   0.414251      0.415675\n",
       "medical_help            0.079493   0.087682      0.080155\n",
       "medical_products        0.050084   0.053962      0.049537\n",
       "search_and_rescue       0.027617   0.028380      0.027820\n",
       "security                0.017966   0.019733      0.017953\n",
       "military                0.032804   0.041552      0.032347\n",
       "water                   0.063778   0.062303      0.064490\n",
       "food                    0.111497   0.098515      0.111179\n",
       "shelter                 0.088267   0.084986      0.088394\n",
       "clothing                0.015449   0.013478      0.015817\n",
       "money                   0.023039   0.023446      0.023090\n",
       "missing_people          0.011367   0.010884      0.010935\n",
       "refugees                0.033377   0.038246      0.031940\n",
       "death                   0.045545   0.050554      0.046638\n",
       "other_aid               0.131446   0.123538      0.131472\n",
       "infrastructure_related  0.065037   0.068355      0.064490\n",
       "transport               0.045812   0.047961      0.046536\n",
       "buildings               0.050847   0.050860      0.051978\n",
       "electricity             0.020293   0.020293      0.019937\n",
       "tools                   0.006065   0.006764      0.006052\n",
       "hospitals               0.010795   0.011647      0.010681\n",
       "shops                   0.004577   0.004425      0.004933\n",
       "aid_centers             0.011787   0.012359      0.011342\n",
       "other_infrastructure    0.043904   0.046740      0.043637\n",
       "weather_related         0.278341   0.278354      0.279575\n",
       "floods                  0.082202   0.093175      0.082901\n",
       "storm                   0.093187   0.091649      0.093327\n",
       "fire                    0.010757   0.012206      0.010477\n",
       "earthquake              0.093645   0.081782      0.093582\n",
       "cold                    0.020217   0.022124      0.020496\n",
       "other_weather           0.052487   0.058285      0.052894\n",
       "direct_report           0.193584   0.157359      0.196216"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset - normalsplit</th>\n",
       "      <th>dataset - iterative split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>related</th>\n",
       "      <td>0.001399</td>\n",
       "      <td>0.000025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>request</th>\n",
       "      <td>0.000381</td>\n",
       "      <td>0.034559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>offer</th>\n",
       "      <td>0.000331</td>\n",
       "      <td>0.000025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aid_related</th>\n",
       "      <td>0.001424</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>medical_help</th>\n",
       "      <td>0.000661</td>\n",
       "      <td>0.008188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>medical_products</th>\n",
       "      <td>0.000547</td>\n",
       "      <td>0.003878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>search_and_rescue</th>\n",
       "      <td>0.000203</td>\n",
       "      <td>0.000763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>security</th>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.001767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>military</th>\n",
       "      <td>0.000458</td>\n",
       "      <td>0.008748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>water</th>\n",
       "      <td>0.000712</td>\n",
       "      <td>0.001475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>food</th>\n",
       "      <td>0.000318</td>\n",
       "      <td>0.012982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>shelter</th>\n",
       "      <td>0.000127</td>\n",
       "      <td>0.003280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>clothing</th>\n",
       "      <td>0.000369</td>\n",
       "      <td>0.001971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>money</th>\n",
       "      <td>0.000051</td>\n",
       "      <td>0.000407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>missing_people</th>\n",
       "      <td>0.000432</td>\n",
       "      <td>0.000483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>refugees</th>\n",
       "      <td>0.001437</td>\n",
       "      <td>0.004870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>death</th>\n",
       "      <td>0.001093</td>\n",
       "      <td>0.005010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>other_aid</th>\n",
       "      <td>0.000025</td>\n",
       "      <td>0.007909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>infrastructure_related</th>\n",
       "      <td>0.000547</td>\n",
       "      <td>0.003319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>transport</th>\n",
       "      <td>0.000725</td>\n",
       "      <td>0.002149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>buildings</th>\n",
       "      <td>0.001132</td>\n",
       "      <td>0.000013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>electricity</th>\n",
       "      <td>0.000356</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tools</th>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.000699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hospitals</th>\n",
       "      <td>0.000114</td>\n",
       "      <td>0.000852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>shops</th>\n",
       "      <td>0.000356</td>\n",
       "      <td>0.000153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aid_centers</th>\n",
       "      <td>0.000445</td>\n",
       "      <td>0.000572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>other_infrastructure</th>\n",
       "      <td>0.000267</td>\n",
       "      <td>0.002835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weather_related</th>\n",
       "      <td>0.001233</td>\n",
       "      <td>0.000013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>floods</th>\n",
       "      <td>0.000699</td>\n",
       "      <td>0.010973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>storm</th>\n",
       "      <td>0.000140</td>\n",
       "      <td>0.001539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fire</th>\n",
       "      <td>0.000280</td>\n",
       "      <td>0.001449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>earthquake</th>\n",
       "      <td>0.000064</td>\n",
       "      <td>0.011863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cold</th>\n",
       "      <td>0.000280</td>\n",
       "      <td>0.001907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>other_weather</th>\n",
       "      <td>0.000407</td>\n",
       "      <td>0.005798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>direct_report</th>\n",
       "      <td>0.002632</td>\n",
       "      <td>0.036225</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        dataset - normalsplit  dataset - iterative split\n",
       "related                              0.001399                   0.000025\n",
       "request                              0.000381                   0.034559\n",
       "offer                                0.000331                   0.000025\n",
       "aid_related                          0.001424                   0.000000\n",
       "medical_help                         0.000661                   0.008188\n",
       "medical_products                     0.000547                   0.003878\n",
       "search_and_rescue                    0.000203                   0.000763\n",
       "security                             0.000013                   0.001767\n",
       "military                             0.000458                   0.008748\n",
       "water                                0.000712                   0.001475\n",
       "food                                 0.000318                   0.012982\n",
       "shelter                              0.000127                   0.003280\n",
       "clothing                             0.000369                   0.001971\n",
       "money                                0.000051                   0.000407\n",
       "missing_people                       0.000432                   0.000483\n",
       "refugees                             0.001437                   0.004870\n",
       "death                                0.001093                   0.005010\n",
       "other_aid                            0.000025                   0.007909\n",
       "infrastructure_related               0.000547                   0.003319\n",
       "transport                            0.000725                   0.002149\n",
       "buildings                            0.001132                   0.000013\n",
       "electricity                          0.000356                   0.000000\n",
       "tools                                0.000013                   0.000699\n",
       "hospitals                            0.000114                   0.000852\n",
       "shops                                0.000356                   0.000153\n",
       "aid_centers                          0.000445                   0.000572\n",
       "other_infrastructure                 0.000267                   0.002835\n",
       "weather_related                      0.001233                   0.000013\n",
       "floods                               0.000699                   0.010973\n",
       "storm                                0.000140                   0.001539\n",
       "fire                                 0.000280                   0.001449\n",
       "earthquake                           0.000064                   0.011863\n",
       "cold                                 0.000280                   0.001907\n",
       "other_weather                        0.000407                   0.005798\n",
       "direct_report                        0.002632                   0.036225"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diff = pd.DataFrame(compare['dataset'] - compare['normal_split'])\n",
    "diff.columns = ['dataset - normalsplit']\n",
    "diff['dataset - iterative split'] = compare['dataset'] - compare['train_set']\n",
    "diff = diff.abs()\n",
    "\n",
    "diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x1000 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "diff.plot(kind='barh', figsize=(10,10))\n",
    "plt.xlabel('Abs Diff in label proportion from original')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that the normal train-test-split does a fine, if not better job, of capturing nearly the same proportion of labels as in the original dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normal train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/james/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('msg_tokenizer',\n",
       "                 MessageTokenizer(lemmatize=True, remove_stops=True)),\n",
       "                ('count_vec',\n",
       "                 CountVectorizer(analyzer='word', binary=False,\n",
       "                                 decode_error='strict',\n",
       "                                 dtype=<class 'numpy.int64'>, encoding='utf-8',\n",
       "                                 input='content', lowercase=True, max_df=1.0,\n",
       "                                 max_features=None, min_df=1,\n",
       "                                 ngram_range=(1, 1), preprocessor=None,\n",
       "                                 stop_words=None, strip_ac...\n",
       "                 RandomForestClassifier(bootstrap=True, class_weight=None,\n",
       "                                        criterion='gini', max_depth=None,\n",
       "                                        max_features='auto',\n",
       "                                        max_leaf_nodes=None,\n",
       "                                        min_impurity_decrease=0.0,\n",
       "                                        min_impurity_split=None,\n",
       "                                        min_samples_leaf=1, min_samples_split=2,\n",
       "                                        min_weight_fraction_leaf=0.0,\n",
       "                                        n_estimators=10, n_jobs=None,\n",
       "                                        oob_score=False, random_state=None,\n",
       "                                        verbose=0, warm_start=False))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit the pipeline!\n",
    "pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MessageTokenizer(remove_stops=True, lemmatize=True).transform(X_train.reshape(-1,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Test the fitted model on the train data\n",
    "We'll save the test data for the very end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = pipeline.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "related \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.98      0.98      4536\n",
      "           1       0.99      1.00      1.00     15126\n",
      "\n",
      "    accuracy                           0.99     19662\n",
      "   macro avg       0.99      0.99      0.99     19662\n",
      "weighted avg       0.99      0.99      0.99     19662\n",
      "\n",
      "request \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      0.99     16508\n",
      "           1       0.93      1.00      0.96      3154\n",
      "\n",
      "    accuracy                           0.99     19662\n",
      "   macro avg       0.96      0.99      0.98     19662\n",
      "weighted avg       0.99      0.99      0.99     19662\n",
      "\n",
      "offer \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     19597\n",
      "           1       0.70      1.00      0.82        65\n",
      "\n",
      "    accuracy                           1.00     19662\n",
      "   macro avg       0.85      1.00      0.91     19662\n",
      "weighted avg       1.00      1.00      1.00     19662\n",
      "\n",
      "aid_related \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.97      0.99     11739\n",
      "           1       0.96      1.00      0.98      7923\n",
      "\n",
      "    accuracy                           0.98     19662\n",
      "   macro avg       0.98      0.99      0.98     19662\n",
      "weighted avg       0.98      0.98      0.98     19662\n",
      "\n",
      "medical_help \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      0.99     18364\n",
      "           1       0.83      1.00      0.91      1298\n",
      "\n",
      "    accuracy                           0.99     19662\n",
      "   macro avg       0.92      0.99      0.95     19662\n",
      "weighted avg       0.99      0.99      0.99     19662\n",
      "\n",
      "medical_products \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      1.00     18833\n",
      "           1       0.84      1.00      0.91       829\n",
      "\n",
      "    accuracy                           0.99     19662\n",
      "   macro avg       0.92      1.00      0.95     19662\n",
      "weighted avg       0.99      0.99      0.99     19662\n",
      "\n",
      "search_and_rescue \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      1.00     19238\n",
      "           1       0.79      1.00      0.88       424\n",
      "\n",
      "    accuracy                           0.99     19662\n",
      "   macro avg       0.90      0.99      0.94     19662\n",
      "weighted avg       1.00      0.99      0.99     19662\n",
      "\n",
      "security \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      1.00     19396\n",
      "           1       0.72      0.99      0.84       266\n",
      "\n",
      "    accuracy                           0.99     19662\n",
      "   macro avg       0.86      0.99      0.92     19662\n",
      "weighted avg       1.00      0.99      1.00     19662\n",
      "\n",
      "military \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      1.00     19102\n",
      "           1       0.83      1.00      0.91       560\n",
      "\n",
      "    accuracy                           0.99     19662\n",
      "   macro avg       0.92      1.00      0.95     19662\n",
      "weighted avg       1.00      0.99      0.99     19662\n",
      "\n",
      "water \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     18477\n",
      "           1       0.94      1.00      0.97      1185\n",
      "\n",
      "    accuracy                           1.00     19662\n",
      "   macro avg       0.97      1.00      0.98     19662\n",
      "weighted avg       1.00      1.00      1.00     19662\n",
      "\n",
      "food \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      1.00     17580\n",
      "           1       0.93      1.00      0.96      2082\n",
      "\n",
      "    accuracy                           0.99     19662\n",
      "   macro avg       0.97      1.00      0.98     19662\n",
      "weighted avg       0.99      0.99      0.99     19662\n",
      "\n",
      "shelter \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      1.00     18096\n",
      "           1       0.90      1.00      0.95      1566\n",
      "\n",
      "    accuracy                           0.99     19662\n",
      "   macro avg       0.95      0.99      0.97     19662\n",
      "weighted avg       0.99      0.99      0.99     19662\n",
      "\n",
      "clothing \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     19408\n",
      "           1       0.82      1.00      0.90       254\n",
      "\n",
      "    accuracy                           1.00     19662\n",
      "   macro avg       0.91      1.00      0.95     19662\n",
      "weighted avg       1.00      1.00      1.00     19662\n",
      "\n",
      "money \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      1.00     19301\n",
      "           1       0.78      1.00      0.87       361\n",
      "\n",
      "    accuracy                           0.99     19662\n",
      "   macro avg       0.89      1.00      0.94     19662\n",
      "weighted avg       1.00      0.99      1.00     19662\n",
      "\n",
      "missing_people \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     19487\n",
      "           1       0.76      0.99      0.86       175\n",
      "\n",
      "    accuracy                           1.00     19662\n",
      "   macro avg       0.88      1.00      0.93     19662\n",
      "weighted avg       1.00      1.00      1.00     19662\n",
      "\n",
      "refugees \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      1.00     19132\n",
      "           1       0.80      1.00      0.89       530\n",
      "\n",
      "    accuracy                           0.99     19662\n",
      "   macro avg       0.90      1.00      0.94     19662\n",
      "weighted avg       0.99      0.99      0.99     19662\n",
      "\n",
      "death \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      1.00     18897\n",
      "           1       0.83      1.00      0.91       765\n",
      "\n",
      "    accuracy                           0.99     19662\n",
      "   macro avg       0.92      1.00      0.95     19662\n",
      "weighted avg       0.99      0.99      0.99     19662\n",
      "\n",
      "other_aid \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99     17496\n",
      "           1       0.84      1.00      0.91      2166\n",
      "\n",
      "    accuracy                           0.98     19662\n",
      "   macro avg       0.92      0.99      0.95     19662\n",
      "weighted avg       0.98      0.98      0.98     19662\n",
      "\n",
      "infrastructure_related \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      0.99     18627\n",
      "           1       0.79      1.00      0.89      1035\n",
      "\n",
      "    accuracy                           0.99     19662\n",
      "   macro avg       0.90      0.99      0.94     19662\n",
      "weighted avg       0.99      0.99      0.99     19662\n",
      "\n",
      "transport \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      1.00     18932\n",
      "           1       0.80      1.00      0.89       730\n",
      "\n",
      "    accuracy                           0.99     19662\n",
      "   macro avg       0.90      0.99      0.94     19662\n",
      "weighted avg       0.99      0.99      0.99     19662\n",
      "\n",
      "buildings \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      1.00     18805\n",
      "           1       0.84      1.00      0.91       857\n",
      "\n",
      "    accuracy                           0.99     19662\n",
      "   macro avg       0.92      0.99      0.95     19662\n",
      "weighted avg       0.99      0.99      0.99     19662\n",
      "\n",
      "electricity \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     19350\n",
      "           1       0.79      1.00      0.88       312\n",
      "\n",
      "    accuracy                           1.00     19662\n",
      "   macro avg       0.90      1.00      0.94     19662\n",
      "weighted avg       1.00      1.00      1.00     19662\n",
      "\n",
      "tools \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     19578\n",
      "           1       0.75      1.00      0.86        84\n",
      "\n",
      "    accuracy                           1.00     19662\n",
      "   macro avg       0.88      1.00      0.93     19662\n",
      "weighted avg       1.00      1.00      1.00     19662\n",
      "\n",
      "hospitals \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     19506\n",
      "           1       0.73      1.00      0.84       156\n",
      "\n",
      "    accuracy                           1.00     19662\n",
      "   macro avg       0.86      1.00      0.92     19662\n",
      "weighted avg       1.00      1.00      1.00     19662\n",
      "\n",
      "shops \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     19594\n",
      "           1       0.72      1.00      0.83        68\n",
      "\n",
      "    accuracy                           1.00     19662\n",
      "   macro avg       0.86      1.00      0.92     19662\n",
      "weighted avg       1.00      1.00      1.00     19662\n",
      "\n",
      "aid_centers \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     19476\n",
      "           1       0.77      1.00      0.87       186\n",
      "\n",
      "    accuracy                           1.00     19662\n",
      "   macro avg       0.88      1.00      0.93     19662\n",
      "weighted avg       1.00      1.00      1.00     19662\n",
      "\n",
      "other_infrastructure \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      1.00     18974\n",
      "           1       0.79      1.00      0.88       688\n",
      "\n",
      "    accuracy                           0.99     19662\n",
      "   macro avg       0.90      0.99      0.94     19662\n",
      "weighted avg       0.99      0.99      0.99     19662\n",
      "\n",
      "weather_related \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99     14374\n",
      "           1       0.95      1.00      0.97      5288\n",
      "\n",
      "    accuracy                           0.99     19662\n",
      "   macro avg       0.97      0.99      0.98     19662\n",
      "weighted avg       0.99      0.99      0.99     19662\n",
      "\n",
      "floods \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      0.99     18184\n",
      "           1       0.89      1.00      0.94      1478\n",
      "\n",
      "    accuracy                           0.99     19662\n",
      "   macro avg       0.94      0.99      0.97     19662\n",
      "weighted avg       0.99      0.99      0.99     19662\n",
      "\n",
      "storm \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      1.00     17976\n",
      "           1       0.91      1.00      0.95      1686\n",
      "\n",
      "    accuracy                           0.99     19662\n",
      "   macro avg       0.95      0.99      0.97     19662\n",
      "weighted avg       0.99      0.99      0.99     19662\n",
      "\n",
      "fire \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     19490\n",
      "           1       0.76      1.00      0.86       172\n",
      "\n",
      "    accuracy                           1.00     19662\n",
      "   macro avg       0.88      1.00      0.93     19662\n",
      "weighted avg       1.00      1.00      1.00     19662\n",
      "\n",
      "earthquake \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     17883\n",
      "           1       0.96      0.99      0.98      1779\n",
      "\n",
      "    accuracy                           1.00     19662\n",
      "   macro avg       0.98      1.00      0.99     19662\n",
      "weighted avg       1.00      1.00      1.00     19662\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cold \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     19339\n",
      "           1       0.81      1.00      0.90       323\n",
      "\n",
      "    accuracy                           1.00     19662\n",
      "   macro avg       0.91      1.00      0.95     19662\n",
      "weighted avg       1.00      1.00      1.00     19662\n",
      "\n",
      "other_weather \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      0.99     18829\n",
      "           1       0.81      1.00      0.89       833\n",
      "\n",
      "    accuracy                           0.99     19662\n",
      "   macro avg       0.90      0.99      0.94     19662\n",
      "weighted avg       0.99      0.99      0.99     19662\n",
      "\n",
      "direct_report \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99     16133\n",
      "           1       0.92      1.00      0.96      3529\n",
      "\n",
      "    accuracy                           0.98     19662\n",
      "   macro avg       0.96      0.99      0.97     19662\n",
      "weighted avg       0.99      0.98      0.98     19662\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# iterate thru all the labels and check the accuracy on each label\n",
    "\n",
    "y_pred = pd.DataFrame(y_pred, columns=Y.columns)\n",
    "y_train = pd.DataFrame(y_train, columns=Y.columns)\n",
    "\n",
    "for col in Y.columns:\n",
    "    print(col, '\\n', classification_report(y_pred[col], y_train[col]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "related \n",
      " [[ 4453    83]\n",
      " [   64 15062]]\n",
      "\n",
      "\n",
      "request \n",
      " [[16271   237]\n",
      " [   11  3143]]\n",
      "\n",
      "\n",
      "offer \n",
      " [[19569    28]\n",
      " [    0    65]]\n",
      "\n",
      "\n",
      "aid_related \n",
      " [[11440   299]\n",
      " [   17  7906]]\n",
      "\n",
      "\n",
      "medical_help \n",
      " [[18099   265]\n",
      " [    1  1297]]\n",
      "\n",
      "\n",
      "medical_products \n",
      " [[18678   155]\n",
      " [    1   828]]\n",
      "\n",
      "\n",
      "search_and_rescue \n",
      " [[19129   109]\n",
      " [    2   422]]\n",
      "\n",
      "\n",
      "security \n",
      " [[19294   102]\n",
      " [    2   264]]\n",
      "\n",
      "\n",
      "military \n",
      " [[18989   113]\n",
      " [    0   560]]\n",
      "\n",
      "\n",
      "water \n",
      " [[18400    77]\n",
      " [    0  1185]]\n",
      "\n",
      "\n",
      "food \n",
      " [[17429   151]\n",
      " [    2  2080]]\n",
      "\n",
      "\n",
      "shelter \n",
      " [[17930   166]\n",
      " [    4  1562]]\n",
      "\n",
      "\n",
      "clothing \n",
      " [[19351    57]\n",
      " [    0   254]]\n",
      "\n",
      "\n",
      "money \n",
      " [[19199   102]\n",
      " [    1   360]]\n",
      "\n",
      "\n",
      "missing_people \n",
      " [[19432    55]\n",
      " [    1   174]]\n",
      "\n",
      "\n",
      "refugees \n",
      " [[18996   136]\n",
      " [    1   529]]\n",
      "\n",
      "\n",
      "death \n",
      " [[18746   151]\n",
      " [    1   764]]\n",
      "\n",
      "\n",
      "other_aid \n",
      " [[17083   413]\n",
      " [    5  2161]]\n",
      "\n",
      "\n",
      "infrastructure_related \n",
      " [[18360   267]\n",
      " [    1  1034]]\n",
      "\n",
      "\n",
      "transport \n",
      " [[18748   184]\n",
      " [    2   728]]\n",
      "\n",
      "\n",
      "buildings \n",
      " [[18637   168]\n",
      " [    1   856]]\n",
      "\n",
      "\n",
      "electricity \n",
      " [[19268    82]\n",
      " [    0   312]]\n",
      "\n",
      "\n",
      "tools \n",
      " [[19550    28]\n",
      " [    0    84]]\n",
      "\n",
      "\n",
      "hospitals \n",
      " [[19448    58]\n",
      " [    0   156]]\n",
      "\n",
      "\n",
      "shops \n",
      " [[19567    27]\n",
      " [    0    68]]\n",
      "\n",
      "\n",
      "aid_centers \n",
      " [[19420    56]\n",
      " [    0   186]]\n",
      "\n",
      "\n",
      "other_infrastructure \n",
      " [[18792   182]\n",
      " [    1   687]]\n",
      "\n",
      "\n",
      "weather_related \n",
      " [[14102   272]\n",
      " [   17  5271]]\n",
      "\n",
      "\n",
      "floods \n",
      " [[18002   182]\n",
      " [    5  1473]]\n",
      "\n",
      "\n",
      "storm \n",
      " [[17805   171]\n",
      " [    4  1682]]\n",
      "\n",
      "\n",
      "fire \n",
      " [[19435    55]\n",
      " [    0   172]]\n",
      "\n",
      "\n",
      "earthquake \n",
      " [[17805    78]\n",
      " [    9  1770]]\n",
      "\n",
      "\n",
      "cold \n",
      " [[19264    75]\n",
      " [    0   323]]\n",
      "\n",
      "\n",
      "other_weather \n",
      " [[18632   197]\n",
      " [    2   831]]\n",
      "\n",
      "\n",
      "direct_report \n",
      " [[15842   291]\n",
      " [    9  3520]]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "for col in Y.columns:\n",
    "    print(col, '\\n', confusion_matrix(y_pred[col], y_train[col]))\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The performance of the classifier on the training data seems fairly good overall.\n",
    "Another way to gauge the results of a multi-label output is by computing the hamming loss.\n",
    "Hamming loss is the fraction of labels that are incorrectly predicted, i.e., the fraction of the wrong labels to the total number of labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hamming loss:  0.00760858508798698\n"
     ]
    }
   ],
   "source": [
    "# compute hamming loss as well\n",
    "from sklearn.metrics import hamming_loss, make_scorer\n",
    "\n",
    "# drop related column while measuring this because there are some rows with related=2\n",
    "print('Hamming loss: ', hamming_loss(y_train, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/james/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('msg_tokenizer',\n",
       "                 MessageTokenizer(lemmatize=True, remove_stops=True)),\n",
       "                ('count_vec',\n",
       "                 CountVectorizer(analyzer='word', binary=False,\n",
       "                                 decode_error='strict',\n",
       "                                 dtype=<class 'numpy.int64'>, encoding='utf-8',\n",
       "                                 input='content', lowercase=True, max_df=1.0,\n",
       "                                 max_features=None, min_df=1,\n",
       "                                 ngram_range=(1, 1), preprocessor=None,\n",
       "                                 stop_words=None, strip_ac...\n",
       "                 RandomForestClassifier(bootstrap=True, class_weight=None,\n",
       "                                        criterion='gini', max_depth=None,\n",
       "                                        max_features='auto',\n",
       "                                        max_leaf_nodes=None,\n",
       "                                        min_impurity_decrease=0.0,\n",
       "                                        min_impurity_split=None,\n",
       "                                        min_samples_leaf=1, min_samples_split=2,\n",
       "                                        min_weight_fraction_leaf=0.0,\n",
       "                                        n_estimators=10, n_jobs=None,\n",
       "                                        oob_score=False, random_state=None,\n",
       "                                        verbose=0, warm_start=False))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit and predict on the iterative split to see if it performs better\n",
    "pipeline2 = Pipeline([\n",
    "    ('msg_tokenizer', MessageTokenizer()),\n",
    "    # Count Vectorizer with Tokenizer\n",
    "    ('count_vec', CountVectorizer()),\n",
    "    # TF-IDF Transformer\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    # classifier - one classifier per label\n",
    "    ('clf', RandomForestClassifier())\n",
    "])\n",
    "\n",
    "X_train2, y_train2, X_test2, y_test2 = iterative_train_test_split(X.values.reshape(-1,1), Y.values, test_size = 0.25)\n",
    "\n",
    "pipeline2.fit(X_train.values.reshape(-1,), y_train2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "related \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.91      0.89      4395\n",
      "           1       0.97      0.96      0.97     15267\n",
      "\n",
      "    accuracy                           0.95     19662\n",
      "   macro avg       0.92      0.94      0.93     19662\n",
      "weighted avg       0.95      0.95      0.95     19662\n",
      "\n",
      "request \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.97      0.98     17459\n",
      "           1       0.79      0.96      0.86      2203\n",
      "\n",
      "    accuracy                           0.97     19662\n",
      "   macro avg       0.89      0.96      0.92     19662\n",
      "weighted avg       0.97      0.97      0.97     19662\n",
      "\n",
      "offer \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     19614\n",
      "           1       0.53      1.00      0.70        48\n",
      "\n",
      "    accuracy                           1.00     19662\n",
      "   macro avg       0.77      1.00      0.85     19662\n",
      "weighted avg       1.00      1.00      1.00     19662\n",
      "\n",
      "aid_related \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.90      0.93     12466\n",
      "           1       0.85      0.96      0.90      7196\n",
      "\n",
      "    accuracy                           0.92     19662\n",
      "   macro avg       0.91      0.93      0.92     19662\n",
      "weighted avg       0.93      0.92      0.92     19662\n",
      "\n",
      "medical_help \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.97      0.98     18572\n",
      "           1       0.63      0.99      0.77      1090\n",
      "\n",
      "    accuracy                           0.97     19662\n",
      "   macro avg       0.81      0.98      0.88     19662\n",
      "weighted avg       0.98      0.97      0.97     19662\n",
      "\n",
      "medical_products \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99     18993\n",
      "           1       0.64      0.99      0.77       669\n",
      "\n",
      "    accuracy                           0.98     19662\n",
      "   macro avg       0.82      0.99      0.88     19662\n",
      "weighted avg       0.99      0.98      0.98     19662\n",
      "\n",
      "search_and_rescue \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      0.99     19323\n",
      "           1       0.59      1.00      0.74       339\n",
      "\n",
      "    accuracy                           0.99     19662\n",
      "   macro avg       0.79      0.99      0.87     19662\n",
      "weighted avg       0.99      0.99      0.99     19662\n",
      "\n",
      "security \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      1.00     19446\n",
      "           1       0.58      0.99      0.73       216\n",
      "\n",
      "    accuracy                           0.99     19662\n",
      "   macro avg       0.79      0.99      0.86     19662\n",
      "weighted avg       1.00      0.99      0.99     19662\n",
      "\n",
      "military \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      0.99     19112\n",
      "           1       0.67      0.99      0.80       550\n",
      "\n",
      "    accuracy                           0.99     19662\n",
      "   macro avg       0.83      0.99      0.90     19662\n",
      "weighted avg       0.99      0.99      0.99     19662\n",
      "\n",
      "water \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      0.99     18667\n",
      "           1       0.79      0.97      0.87       995\n",
      "\n",
      "    accuracy                           0.99     19662\n",
      "   macro avg       0.90      0.98      0.93     19662\n",
      "weighted avg       0.99      0.99      0.99     19662\n",
      "\n",
      "food \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99     18092\n",
      "           1       0.78      0.97      0.87      1570\n",
      "\n",
      "    accuracy                           0.98     19662\n",
      "   macro avg       0.89      0.97      0.93     19662\n",
      "weighted avg       0.98      0.98      0.98     19662\n",
      "\n",
      "shelter \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.97      0.99     18412\n",
      "           1       0.72      0.98      0.83      1250\n",
      "\n",
      "    accuracy                           0.97     19662\n",
      "   macro avg       0.86      0.98      0.91     19662\n",
      "weighted avg       0.98      0.97      0.98     19662\n",
      "\n",
      "clothing \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     19487\n",
      "           1       0.64      0.97      0.77       175\n",
      "\n",
      "    accuracy                           0.99     19662\n",
      "   macro avg       0.82      0.98      0.88     19662\n",
      "weighted avg       1.00      0.99      1.00     19662\n",
      "\n",
      "money \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      1.00     19373\n",
      "           1       0.61      0.99      0.75       289\n",
      "\n",
      "    accuracy                           0.99     19662\n",
      "   macro avg       0.80      0.99      0.87     19662\n",
      "weighted avg       0.99      0.99      0.99     19662\n",
      "\n",
      "missing_people \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     19528\n",
      "           1       0.61      0.99      0.76       134\n",
      "\n",
      "    accuracy                           1.00     19662\n",
      "   macro avg       0.81      0.99      0.88     19662\n",
      "weighted avg       1.00      1.00      1.00     19662\n",
      "\n",
      "refugees \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      0.99     19200\n",
      "           1       0.62      0.99      0.76       462\n",
      "\n",
      "    accuracy                           0.99     19662\n",
      "   macro avg       0.81      0.99      0.88     19662\n",
      "weighted avg       0.99      0.99      0.99     19662\n",
      "\n",
      "death \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99     19025\n",
      "           1       0.65      1.00      0.79       637\n",
      "\n",
      "    accuracy                           0.98     19662\n",
      "   macro avg       0.83      0.99      0.89     19662\n",
      "weighted avg       0.99      0.98      0.98     19662\n",
      "\n",
      "other_aid \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.95      0.97     18116\n",
      "           1       0.63      0.99      0.77      1546\n",
      "\n",
      "    accuracy                           0.95     19662\n",
      "   macro avg       0.82      0.97      0.87     19662\n",
      "weighted avg       0.97      0.95      0.96     19662\n",
      "\n",
      "infrastructure_related \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.97      0.99     18847\n",
      "           1       0.61      1.00      0.76       815\n",
      "\n",
      "    accuracy                           0.97     19662\n",
      "   macro avg       0.81      0.98      0.87     19662\n",
      "weighted avg       0.98      0.97      0.98     19662\n",
      "\n",
      "transport \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99     19081\n",
      "           1       0.62      0.99      0.76       581\n",
      "\n",
      "    accuracy                           0.98     19662\n",
      "   macro avg       0.81      0.99      0.88     19662\n",
      "weighted avg       0.99      0.98      0.98     19662\n",
      "\n",
      "buildings \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99     19004\n",
      "           1       0.65      0.99      0.79       658\n",
      "\n",
      "    accuracy                           0.98     19662\n",
      "   macro avg       0.83      0.99      0.89     19662\n",
      "weighted avg       0.99      0.98      0.98     19662\n",
      "\n",
      "electricity \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      1.00     19426\n",
      "           1       0.59      1.00      0.74       236\n",
      "\n",
      "    accuracy                           0.99     19662\n",
      "   macro avg       0.80      1.00      0.87     19662\n",
      "weighted avg       1.00      0.99      0.99     19662\n",
      "\n",
      "tools \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     19590\n",
      "           1       0.53      1.00      0.69        72\n",
      "\n",
      "    accuracy                           1.00     19662\n",
      "   macro avg       0.76      1.00      0.84     19662\n",
      "weighted avg       1.00      1.00      1.00     19662\n",
      "\n",
      "hospitals \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      1.00     19539\n",
      "           1       0.55      1.00      0.71       123\n",
      "\n",
      "    accuracy                           0.99     19662\n",
      "   macro avg       0.78      1.00      0.85     19662\n",
      "weighted avg       1.00      0.99      1.00     19662\n",
      "\n",
      "shops \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     19615\n",
      "           1       0.54      1.00      0.70        47\n",
      "\n",
      "    accuracy                           1.00     19662\n",
      "   macro avg       0.77      1.00      0.85     19662\n",
      "weighted avg       1.00      1.00      1.00     19662\n",
      "\n",
      "aid_centers \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      1.00     19519\n",
      "           1       0.59      1.00      0.74       143\n",
      "\n",
      "    accuracy                           0.99     19662\n",
      "   macro avg       0.80      1.00      0.87     19662\n",
      "weighted avg       1.00      0.99      1.00     19662\n",
      "\n",
      "other_infrastructure \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99     19114\n",
      "           1       0.60      1.00      0.75       548\n",
      "\n",
      "    accuracy                           0.98     19662\n",
      "   macro avg       0.80      0.99      0.87     19662\n",
      "weighted avg       0.99      0.98      0.98     19662\n",
      "\n",
      "weather_related \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.94      0.97     15002\n",
      "           1       0.83      0.98      0.90      4660\n",
      "\n",
      "    accuracy                           0.95     19662\n",
      "   macro avg       0.91      0.96      0.93     19662\n",
      "weighted avg       0.95      0.95      0.95     19662\n",
      "\n",
      "floods \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99     18219\n",
      "           1       0.77      0.98      0.86      1443\n",
      "\n",
      "    accuracy                           0.98     19662\n",
      "   macro avg       0.89      0.98      0.93     19662\n",
      "weighted avg       0.98      0.98      0.98     19662\n",
      "\n",
      "storm \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.97      0.99     18307\n",
      "           1       0.73      0.97      0.83      1355\n",
      "\n",
      "    accuracy                           0.97     19662\n",
      "   macro avg       0.86      0.97      0.91     19662\n",
      "weighted avg       0.98      0.97      0.97     19662\n",
      "\n",
      "fire \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     19520\n",
      "           1       0.63      1.00      0.77       142\n",
      "\n",
      "    accuracy                           1.00     19662\n",
      "   macro avg       0.81      1.00      0.88     19662\n",
      "weighted avg       1.00      1.00      1.00     19662\n",
      "\n",
      "earthquake \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      0.99     18317\n",
      "           1       0.83      0.98      0.90      1345\n",
      "\n",
      "    accuracy                           0.98     19662\n",
      "   macro avg       0.91      0.98      0.95     19662\n",
      "weighted avg       0.99      0.98      0.99     19662\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cold \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      1.00     19394\n",
      "           1       0.62      0.99      0.76       268\n",
      "\n",
      "    accuracy                           0.99     19662\n",
      "   macro avg       0.81      0.99      0.88     19662\n",
      "weighted avg       0.99      0.99      0.99     19662\n",
      "\n",
      "other_weather \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99     18949\n",
      "           1       0.62      0.99      0.76       713\n",
      "\n",
      "    accuracy                           0.98     19662\n",
      "   macro avg       0.81      0.98      0.87     19662\n",
      "weighted avg       0.99      0.98      0.98     19662\n",
      "\n",
      "direct_report \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.95      0.97     17209\n",
      "           1       0.75      0.96      0.84      2453\n",
      "\n",
      "    accuracy                           0.96     19662\n",
      "   macro avg       0.87      0.96      0.91     19662\n",
      "weighted avg       0.96      0.96      0.96     19662\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred2 = pipeline.predict(X_train2.reshape(-1,))\n",
    "y_pred2 = pd.DataFrame(y_pred2, columns=Y.columns)\n",
    "y_train2 = pd.DataFrame(y_train2, columns=Y.columns)\n",
    "\n",
    "for col in Y.columns:\n",
    "    print(col, '\\n', classification_report(y_pred2[col], y_train2[col]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.02043826380109566"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hamming_loss(y_train2, y_pred2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, doesn't seem to be any advantage to it - precision is lower and hamming loss is higher. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test three different classifiers before tuning hyper params\n",
    "\n",
    "We will test out a \n",
    "1. Multilayer Perceptron - Directly supports multi-label - <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html#sklearn.neural_network.MLPClassifier\"> link </a>\n",
    "2. RidgeClassifierCV <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.RidgeClassifierCV.html#sklearn.linear_model.RidgeClassifierCV\"> link </a>\n",
    "3. A LinearSVC wrapped in OneVsRestClassifier. In essence, train a separate model to predict for each individual label. <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html\"> LinearSVC link </a> and <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.multiclass.OneVsRestClassifier.html\"> OneVsRest link </a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import them\n",
    "\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import RidgeClassifierCV\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.multiclass import OneVsRestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a quick and dirty function to look at model results\n",
    "\n",
    "def model_compare(y_pred, y_train):\n",
    "    y_pred2 = pd.DataFrame(y_pred, columns=Y.columns)\n",
    "    y_train2 = pd.DataFrame(y_train, columns=Y.columns)\n",
    "\n",
    "    for col in Y.columns:\n",
    "        print(col, '\\n', classification_report(y_pred2[col], y_train2[col]))\n",
    "        \n",
    "        print('\\n', confusion_matrix(y_pred2[col], y_train2[col]))\n",
    "        \n",
    "    print(f\"Hamming Loss: {hamming_loss(y_pred, y_train)}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "related \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.88      0.80      3751\n",
      "           1       0.97      0.92      0.95     15911\n",
      "\n",
      "    accuracy                           0.92     19662\n",
      "   macro avg       0.85      0.90      0.87     19662\n",
      "weighted avg       0.93      0.92      0.92     19662\n",
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "multilabel-indicator is not supported",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-7b351524c071>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0my_pred_svc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpipeline_svc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mmodel_compare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred_svc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-35-b68acb1765af>\u001b[0m in \u001b[0;36mmodel_compare\u001b[0;34m(y_pred, y_train)\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'\\n'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclassification_report\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Hamming Loss: {hamming_loss(y_pred, y_train)}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py\u001b[0m in \u001b[0;36mconfusion_matrix\u001b[0;34m(y_true, y_pred, labels, sample_weight)\u001b[0m\n\u001b[1;32m    253\u001b[0m     \u001b[0my_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my_type\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"binary\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"multiclass\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 255\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"%s is not supported\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0my_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: multilabel-indicator is not supported"
     ]
    }
   ],
   "source": [
    "pipeline_svc = Pipeline([\n",
    "    ('msg_tokenizer', MessageTokenizer()),\n",
    "    # Count Vectorizer with Tokenizer\n",
    "    ('count_vec', CountVectorizer()),\n",
    "    # TF-IDF Transformer\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    # classifier - one classifier per label\n",
    "    ('clf', OneVsRestClassifier(SVC(kernel='linear')))\n",
    "])\n",
    "\n",
    "pipeline_svc.fit(X_train, y_train)\n",
    "y_pred_svc = pipeline_svc.predict(X_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "related \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.88      0.80      3751\n",
      "           1       0.97      0.92      0.95     15911\n",
      "\n",
      "    accuracy                           0.92     19662\n",
      "   macro avg       0.85      0.90      0.87     19662\n",
      "weighted avg       0.93      0.92      0.92     19662\n",
      "\n",
      "\n",
      " [[ 3311   440]\n",
      " [ 1206 14705]]\n",
      "request \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.94      0.96     17094\n",
      "           1       0.68      0.90      0.78      2568\n",
      "\n",
      "    accuracy                           0.93     19662\n",
      "   macro avg       0.83      0.92      0.87     19662\n",
      "weighted avg       0.94      0.93      0.94     19662\n",
      "\n",
      "\n",
      " [[16025  1069]\n",
      " [  257  2311]]\n",
      "offer \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     19662\n",
      "           1       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           1.00     19662\n",
      "   macro avg       0.50      0.50      0.50     19662\n",
      "weighted avg       1.00      1.00      1.00     19662\n",
      "\n",
      "\n",
      " [[19569    93]\n",
      " [    0     0]]\n",
      "aid_related \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.88      0.90     12058\n",
      "           1       0.82      0.88      0.85      7604\n",
      "\n",
      "    accuracy                           0.88     19662\n",
      "   macro avg       0.87      0.88      0.87     19662\n",
      "weighted avg       0.88      0.88      0.88     19662\n",
      "\n",
      "\n",
      " [[10554  1504]\n",
      " [  903  6701]]\n",
      "medical_help \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.95      0.97     18992\n",
      "           1       0.38      0.90      0.54       670\n",
      "\n",
      "    accuracy                           0.95     19662\n",
      "   macro avg       0.69      0.92      0.76     19662\n",
      "weighted avg       0.98      0.95      0.96     19662\n",
      "\n",
      "\n",
      " [[18031   961]\n",
      " [   69   601]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/james/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1439: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "medical_products \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.97      0.98     19239\n",
      "           1       0.38      0.88      0.53       423\n",
      "\n",
      "    accuracy                           0.97     19662\n",
      "   macro avg       0.69      0.92      0.76     19662\n",
      "weighted avg       0.98      0.97      0.97     19662\n",
      "\n",
      "\n",
      " [[18628   611]\n",
      " [   51   372]]\n",
      "search_and_rescue \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99     19514\n",
      "           1       0.25      0.89      0.39       148\n",
      "\n",
      "    accuracy                           0.98     19662\n",
      "   macro avg       0.62      0.93      0.69     19662\n",
      "weighted avg       0.99      0.98      0.98     19662\n",
      "\n",
      "\n",
      " [[19114   400]\n",
      " [   17   131]]\n",
      "security \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99     19661\n",
      "           1       0.00      1.00      0.01         1\n",
      "\n",
      "    accuracy                           0.98     19662\n",
      "   macro avg       0.50      0.99      0.50     19662\n",
      "weighted avg       1.00      0.98      0.99     19662\n",
      "\n",
      "\n",
      " [[19296   365]\n",
      " [    0     1]]\n",
      "military \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      0.99     19235\n",
      "           1       0.59      0.93      0.73       427\n",
      "\n",
      "    accuracy                           0.98     19662\n",
      "   macro avg       0.80      0.96      0.86     19662\n",
      "weighted avg       0.99      0.98      0.99     19662\n",
      "\n",
      "\n",
      " [[18961   274]\n",
      " [   28   399]]\n",
      "water \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99     18518\n",
      "           1       0.82      0.90      0.86      1144\n",
      "\n",
      "    accuracy                           0.98     19662\n",
      "   macro avg       0.91      0.95      0.92     19662\n",
      "weighted avg       0.98      0.98      0.98     19662\n",
      "\n",
      "\n",
      " [[18289   229]\n",
      " [  111  1033]]\n",
      "food \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.98      0.98     17588\n",
      "           1       0.82      0.89      0.85      2074\n",
      "\n",
      "    accuracy                           0.97     19662\n",
      "   macro avg       0.91      0.93      0.92     19662\n",
      "weighted avg       0.97      0.97      0.97     19662\n",
      "\n",
      "\n",
      " [[17195   393]\n",
      " [  236  1838]]\n",
      "shelter \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.97      0.98     18386\n",
      "           1       0.66      0.89      0.76      1276\n",
      "\n",
      "    accuracy                           0.96     19662\n",
      "   macro avg       0.83      0.93      0.87     19662\n",
      "weighted avg       0.97      0.96      0.97     19662\n",
      "\n",
      "\n",
      " [[17798   588]\n",
      " [  136  1140]]\n",
      "clothing \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      1.00     19450\n",
      "           1       0.60      0.88      0.72       212\n",
      "\n",
      "    accuracy                           0.99     19662\n",
      "   macro avg       0.80      0.94      0.86     19662\n",
      "weighted avg       0.99      0.99      0.99     19662\n",
      "\n",
      "\n",
      " [[19326   124]\n",
      " [   25   187]]\n",
      "money \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99     19498\n",
      "           1       0.34      0.97      0.51       164\n",
      "\n",
      "    accuracy                           0.98     19662\n",
      "   macro avg       0.67      0.98      0.75     19662\n",
      "weighted avg       0.99      0.98      0.99     19662\n",
      "\n",
      "\n",
      " [[19195   303]\n",
      " [    5   159]]\n",
      "missing_people \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      1.00     19609\n",
      "           1       0.23      0.98      0.37        53\n",
      "\n",
      "    accuracy                           0.99     19662\n",
      "   macro avg       0.61      0.99      0.68     19662\n",
      "weighted avg       1.00      0.99      0.99     19662\n",
      "\n",
      "\n",
      " [[19432   177]\n",
      " [    1    52]]\n",
      "refugees \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99     19438\n",
      "           1       0.31      0.92      0.46       224\n",
      "\n",
      "    accuracy                           0.98     19662\n",
      "   macro avg       0.65      0.95      0.72     19662\n",
      "weighted avg       0.99      0.98      0.98     19662\n",
      "\n",
      "\n",
      " [[18978   460]\n",
      " [   19   205]]\n",
      "death \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99     19014\n",
      "           1       0.65      0.92      0.77       648\n",
      "\n",
      "    accuracy                           0.98     19662\n",
      "   macro avg       0.83      0.95      0.88     19662\n",
      "weighted avg       0.99      0.98      0.98     19662\n",
      "\n",
      "\n",
      " [[18698   316]\n",
      " [   49   599]]\n",
      "other_aid \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.89      0.94     19186\n",
      "           1       0.17      0.92      0.29       476\n",
      "\n",
      "    accuracy                           0.89     19662\n",
      "   macro avg       0.58      0.91      0.61     19662\n",
      "weighted avg       0.98      0.89      0.92     19662\n",
      "\n",
      "\n",
      " [[17051  2135]\n",
      " [   37   439]]\n",
      "infrastructure_related \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.94      0.97     19615\n",
      "           1       0.04      0.98      0.07        47\n",
      "\n",
      "    accuracy                           0.94     19662\n",
      "   macro avg       0.52      0.96      0.52     19662\n",
      "weighted avg       1.00      0.94      0.96     19662\n",
      "\n",
      "\n",
      " [[18360  1255]\n",
      " [    1    46]]\n",
      "transport \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.96      0.98     19420\n",
      "           1       0.25      0.95      0.40       242\n",
      "\n",
      "    accuracy                           0.96     19662\n",
      "   macro avg       0.63      0.96      0.69     19662\n",
      "weighted avg       0.99      0.96      0.97     19662\n",
      "\n",
      "\n",
      " [[18737   683]\n",
      " [   13   229]]\n",
      "buildings \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.97      0.99     19092\n",
      "           1       0.52      0.94      0.67       570\n",
      "\n",
      "    accuracy                           0.97     19662\n",
      "   macro avg       0.76      0.96      0.83     19662\n",
      "weighted avg       0.98      0.97      0.98     19662\n",
      "\n",
      "\n",
      " [[18604   488]\n",
      " [   34   536]]\n",
      "electricity \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      0.99     19472\n",
      "           1       0.46      0.96      0.62       190\n",
      "\n",
      "    accuracy                           0.99     19662\n",
      "   macro avg       0.73      0.97      0.81     19662\n",
      "weighted avg       0.99      0.99      0.99     19662\n",
      "\n",
      "\n",
      " [[19260   212]\n",
      " [    8   182]]\n",
      "tools \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      1.00     19662\n",
      "           1       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.99     19662\n",
      "   macro avg       0.50      0.50      0.50     19662\n",
      "weighted avg       1.00      0.99      1.00     19662\n",
      "\n",
      "\n",
      " [[19550   112]\n",
      " [    0     0]]\n",
      "hospitals \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      0.99     19651\n",
      "           1       0.05      1.00      0.10        11\n",
      "\n",
      "    accuracy                           0.99     19662\n",
      "   macro avg       0.53      0.99      0.55     19662\n",
      "weighted avg       1.00      0.99      0.99     19662\n",
      "\n",
      "\n",
      " [[19448   203]\n",
      " [    0    11]]\n",
      "shops \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     19662\n",
      "           1       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           1.00     19662\n",
      "   macro avg       0.50      0.50      0.50     19662\n",
      "weighted avg       1.00      1.00      1.00     19662\n",
      "\n",
      "\n",
      " [[19567    95]\n",
      " [    0     0]]\n",
      "aid_centers \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      0.99     19661\n",
      "           1       0.00      1.00      0.01         1\n",
      "\n",
      "    accuracy                           0.99     19662\n",
      "   macro avg       0.50      0.99      0.50     19662\n",
      "weighted avg       1.00      0.99      0.99     19662\n",
      "\n",
      "\n",
      " [[19420   241]\n",
      " [    0     1]]\n",
      "other_infrastructure \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.96      0.98     19651\n",
      "           1       0.01      1.00      0.02        11\n",
      "\n",
      "    accuracy                           0.96     19662\n",
      "   macro avg       0.51      0.98      0.50     19662\n",
      "weighted avg       1.00      0.96      0.98     19662\n",
      "\n",
      "\n",
      " [[18793   858]\n",
      " [    0    11]]\n",
      "weather_related \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.93      0.95     14753\n",
      "           1       0.82      0.92      0.87      4909\n",
      "\n",
      "    accuracy                           0.93     19662\n",
      "   macro avg       0.90      0.93      0.91     19662\n",
      "weighted avg       0.94      0.93      0.93     19662\n",
      "\n",
      "\n",
      " [[13749  1004]\n",
      " [  370  4539]]\n",
      "floods \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.96      0.98     18612\n",
      "           1       0.60      0.95      0.73      1050\n",
      "\n",
      "    accuracy                           0.96     19662\n",
      "   macro avg       0.80      0.96      0.86     19662\n",
      "weighted avg       0.98      0.96      0.97     19662\n",
      "\n",
      "\n",
      " [[17951   661]\n",
      " [   56   994]]\n",
      "storm \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.98      0.98     18020\n",
      "           1       0.80      0.90      0.85      1642\n",
      "\n",
      "    accuracy                           0.97     19662\n",
      "   macro avg       0.89      0.94      0.92     19662\n",
      "weighted avg       0.97      0.97      0.97     19662\n",
      "\n",
      "\n",
      " [[17644   376]\n",
      " [  165  1477]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fire \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      1.00     19556\n",
      "           1       0.45      0.96      0.61       106\n",
      "\n",
      "    accuracy                           0.99     19662\n",
      "   macro avg       0.72      0.98      0.80     19662\n",
      "weighted avg       1.00      0.99      0.99     19662\n",
      "\n",
      "\n",
      " [[19431   125]\n",
      " [    4   102]]\n",
      "earthquake \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.98      0.99     17992\n",
      "           1       0.84      0.93      0.88      1670\n",
      "\n",
      "    accuracy                           0.98     19662\n",
      "   macro avg       0.92      0.96      0.93     19662\n",
      "weighted avg       0.98      0.98      0.98     19662\n",
      "\n",
      "\n",
      " [[17693   299]\n",
      " [  121  1549]]\n",
      "cold \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      0.99     19429\n",
      "           1       0.54      0.92      0.68       233\n",
      "\n",
      "    accuracy                           0.99     19662\n",
      "   macro avg       0.77      0.96      0.84     19662\n",
      "weighted avg       0.99      0.99      0.99     19662\n",
      "\n",
      "\n",
      " [[19246   183]\n",
      " [   18   215]]\n",
      "other_weather \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.96      0.98     19453\n",
      "           1       0.19      0.93      0.32       209\n",
      "\n",
      "    accuracy                           0.96     19662\n",
      "   macro avg       0.59      0.95      0.65     19662\n",
      "weighted avg       0.99      0.96      0.97     19662\n",
      "\n",
      "\n",
      " [[18620   833]\n",
      " [   14   195]]\n",
      "direct_report \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.91      0.94     17011\n",
      "           1       0.61      0.87      0.71      2651\n",
      "\n",
      "    accuracy                           0.91     19662\n",
      "   macro avg       0.79      0.89      0.83     19662\n",
      "weighted avg       0.93      0.91      0.91     19662\n",
      "\n",
      "\n",
      " [[15506  1505]\n",
      " [  345  2306]]\n",
      "Hamming Loss: 0.03469200924190244\n"
     ]
    }
   ],
   "source": [
    "model_compare(y_pred_svc, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-55-4bb104eca0a6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m ])\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mpipeline_ridge\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0my_pred_ridge\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpipeline_rf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    354\u001b[0m                                  self._log_message(len(self.steps) - 1)):\n\u001b[1;32m    355\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_final_estimator\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'passthrough'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 356\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_final_estimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    357\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/ridge.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m   1813\u001b[0m                              compute_sample_weight(self.class_weight, y))\n\u001b[1;32m   1814\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1815\u001b[0;31m         \u001b[0m_BaseRidgeCV\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1816\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1817\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/ridge.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m   1526\u001b[0m                                   \u001b[0mgcv_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgcv_mode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1527\u001b[0m                                   store_cv_values=self.store_cv_values)\n\u001b[0;32m-> 1528\u001b[0;31m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1529\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malpha_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malpha_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstore_cv_values\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/ridge.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m   1434\u001b[0m                              dtype=X.dtype)\n\u001b[1;32m   1435\u001b[0m         \u001b[0mC\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1436\u001b[0;31m         \u001b[0mX_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mdecomposition\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecompose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msqrt_sw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1437\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malphas\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1438\u001b[0m             G_diag, c = solve(\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/ridge.py\u001b[0m in \u001b[0;36m_eigen_decompose_gram\u001b[0;34m(self, X, y, sqrt_sw)\u001b[0m\n\u001b[1;32m   1220\u001b[0m         \u001b[0;34m\"\"\"Eigendecomposition of X.X^T, used when n_samples <= n_features\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1221\u001b[0m         \u001b[0;31m# if X is dense it has already been centered in preprocessing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1222\u001b[0;31m         \u001b[0mK\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_mean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compute_gram\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msqrt_sw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1223\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_intercept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1224\u001b[0m             \u001b[0;31m# to emulate centering X with sample weights,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/ridge.py\u001b[0m in \u001b[0;36m_compute_gram\u001b[0;34m(self, X, sqrt_sw)\u001b[0m\n\u001b[1;32m   1133\u001b[0m         \u001b[0mX_mX_m\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mouter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msqrt_sw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msqrt_sw\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_mean\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m         return (safe_sparse_dot(X, X.T, dense_output=True) + X_mX_m\n\u001b[0;32m-> 1135\u001b[0;31m                 - X_mX - X_mX.T, X_mean)\n\u001b[0m\u001b[1;32m   1136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1137\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_compute_covariance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msqrt_sw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/utils/extmath.py\u001b[0m in \u001b[0;36msafe_sparse_dot\u001b[0;34m(a, b, dense_output)\u001b[0m\n\u001b[1;32m    137\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdense_output\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"toarray\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/scipy/sparse/compressed.py\u001b[0m in \u001b[0;36mtoarray\u001b[0;34m(self, order, out)\u001b[0m\n\u001b[1;32m   1022\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0morder\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m             \u001b[0morder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_swap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cf'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1024\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_toarray_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1025\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_contiguous\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf_contiguous\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1026\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Output array must be C or F contiguous'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/scipy/sparse/base.py\u001b[0m in \u001b[0;36m_process_toarray_args\u001b[0;34m(self, order, out)\u001b[0m\n\u001b[1;32m   1184\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1185\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1186\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1187\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# RidgeClassifierCV supports direct multi label output - don't have to wrap in a MultiOutputClassifier\n",
    "\n",
    "pipeline_ridge = Pipeline([\n",
    "    ('msg_tokenizer', MessageTokenizer()),\n",
    "    # Count Vectorizer with Tokenizer\n",
    "    ('count_vec', CountVectorizer(tokenizer=tokenize)),\n",
    "    # TF-IDF Transformer\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    # classifier - one classifier per label\n",
    "    ('clf', RidgeClassifierCV())\n",
    "])\n",
    "\n",
    "pipeline_ridge.fit(X_train, y_train)\n",
    "y_pred_ridge = pipeline_rf.predict(X_train)\n",
    "\n",
    "model_compare(y_pred_ridge, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "JW where i left off\n",
    "I should plot precision vs recall\n",
    "this is a situation where our recall, measure of actual positives that are correct, is super high but the actual\n",
    "positive class appearances are very infrequent. \n",
    "Therefore it would make sense to relax the recall, ie relax the predict_proba boundary in order to capture more\n",
    "of the actual positives (increase precision) while letting in some false positives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "del(pipeline_ridge)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0.8% of labels are predicted incorrectly with RF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The SVC Classifier is not as good at distinguishing between the majority and the minority classes. Accuracy on the individual labels is worse, and Hamming score is worse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tune the grid with hamming loss  - https://scikit-learn.org/stable/modules/generated/sklearn.metrics.hamming_loss.html\n",
    "# make a custom scorer https://scikit-learn.org/stable/modules/model_evaluation.html#scoring\n",
    "from sklearn.metrics import hamming_loss, make_scorer\n",
    "hamming_scorer = make_scorer(hamming_loss, greater_is_better=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a DenseTranformer class to make sure the output is correct for the GaussianNB\n",
    "\n",
    "class DenseTransformer(TransformerMixin):\n",
    "\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None, **fit_params):\n",
    "        return X.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_nb = Pipeline([\n",
    "    ('msg_tokenizer', MessageTokenizer()),\n",
    "    # Count Vectorizer with Tokenizer\n",
    "    ('count_vec', CountVectorizer(tokenizer=tokenize)),\n",
    "    # TF-IDF Transformer\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    # make sure the output is dense, not sparse - needed for GaussianNB\n",
    "    ('dense', DenseTransformer()),\n",
    "    # classifier - one classifier per label\n",
    "    ('clf', MultiOutputClassifier(estimator=GaussianNB()))\n",
    "])\n",
    "\n",
    "pipeline_nb.fit(X_train.values, y_train)\n",
    "y_pred_nb = pipeline_nb.predict(X_train.values)\n",
    "\n",
    "model_compare(y_pred_nb, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choosing to go with the RandomForest model because it seems to yield the best results on training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Improve your model\n",
    "Use grid search to find better parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'RandomizedSearchCV' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-ae2fec35b5c5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m }\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mcv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRandomizedSearchCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpipeline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msearch_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhamming_scorer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0msearch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'RandomizedSearchCV' is not defined"
     ]
    }
   ],
   "source": [
    "search_params = {\n",
    "    'msg_tokenizer__remove_stops': [False, True],\n",
    "    'msg_tokenizer__lemmatize': [False, True],\n",
    "    'count_vec__ngram_range': [(1,1), (1,2), (1,3)],\n",
    "    'count_vec__max_features': [None, 100, 500, 1000],\n",
    "    'tfidf__norm': [None, 'l1', 'l2'],\n",
    "    'tfidf__use_idf': [False, True],\n",
    "    'tfidf__smooth_idf': [False, True],\n",
    "    'clf__estimator__n_estimators': [10, 100, 500],\n",
    "    'clf__estimator__max_depth': [None, 50, 100, 500],\n",
    "    'clf__estimator__bootstrap': [True, False],\n",
    "    'clf__estimator__class_weight': [None, 'balanced']\n",
    "}\n",
    "\n",
    "cv = RandomizedSearchCV(pipeline, search_params, n_iter=5, n_jobs=-1, scoring=hamming_scorer)\n",
    "search = cv.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2 = RandomizedSearchCV(pipeline, search_params, n_iter=10, n_jobs=-1, scoring=hamming_score)\n",
    "search2 = cv.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Test your model\n",
    "Show the accuracy, precision, and recall of the tuned model.  \n",
    "\n",
    "Since this project focuses on code quality, process, and  pipelines, there is no minimum performance metric needed to pass. However, make sure to fine tune your models for accuracy, precision and recall to make your project stand out - especially for your portfolio!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Try improving your model further. Here are a few ideas:\n",
    "* try other machine learning algorithms\n",
    "* add other features besides the TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Export your model as a pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Use this notebook to complete `train.py`\n",
    "Use the template file attached in the Resources folder to write a script that runs the steps above to create a database and export a model based on a new dataset specified by the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
